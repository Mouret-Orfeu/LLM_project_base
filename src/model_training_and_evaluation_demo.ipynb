{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell only if you are on Google Colab and such\n",
        "!git clone https://github.com/Mouret-Orfeu/LLM_project_base.git\n",
        "%cd LLM_PROJECT_BASE\n",
        "\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e .\n",
        "\n",
        "# restart runtime, so that the environment changes are applied\n",
        "# it raises an error \"session crashed for unknown reason\" but it is expected\n",
        "import os, sys\n",
        "os.kill(os.getpid(), 9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# then move to the colab root directory of the project (if working there)\n",
        "%cd /content/LLM_PROJECT_BASE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run this cell if you are running this notebook locally\n",
        "import os\n",
        "os.chdir('/home/orfeu/Documents/documents/info_perso/LLM_PROJECT_BASE') # path to the root of the project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CA1bZzUoKVCG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CWD: /home/orfeu/Documents/documents/info_perso/LLM_project_base\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "# Sanity check\n",
        "print(\"CWD:\", os.getcwd())\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from llm_custom_lib.utils import set_seed\n",
        "from llm_custom_lib.CustomDataset import CustomDataset\n",
        "from llm_custom_lib.Trainer import Trainer\n",
        "from llm_custom_lib.HFModelAdapter import HFModelAdapter\n",
        "from llm_custom_lib.Evaluator import Evaluator\n",
        "set_seed(3407)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s4Zdi_HTKVCG"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "69b400ceb40e4aa59730745c6f4330cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# connect to Hugging face to acces the HF model\n",
        "#!pip install -U \"huggingface_hub[cli]\"\n",
        "from huggingface_hub import login\n",
        "login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RUf-Xz6oKVCJ"
      },
      "outputs": [],
      "source": [
        "# Example code to choose between two models\n",
        "# model_id_1 = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "# model_id_2 = \"meta-llama/Llama-3.2-1B\"\n",
        "\n",
        "# # choose the model you want to use\n",
        "# user_input = input(\"Enter 1 for meta-llama/Llama-3.2-1B-Instruct \\nor enter 2 for the second model\")\n",
        "# if user_input == \"1\":\n",
        "#     model_id = model_id_1\n",
        "# elif user_input == \"2\":\n",
        "#     model_id = model_id_2\n",
        "# else:\n",
        "#     print(\"Invalid input. Please enter 1 or 2.\")\n",
        "#     sys.exit(1)\n",
        "\n",
        "#model_id = \"mistral-community/Mixtral-8x22B-v0.1\" # choose your HF model here\n",
        "model_id = \"Qwen/Qwen3-0.6B\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a5upXxDEKVCJ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "950e10cd58004e72811b18851e94d76d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6f40078f1388444cb8c19abb2329e859",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_id, token=True)\n",
        "hf_model = AutoModelForCausalLM.from_pretrained(model_id, token=True, torch_dtype=\"float16\", device_map=\"auto\")\n",
        "# if the mistral model does not fit, try commenting previous line and running this one instead (to use reduced precision):\n",
        "# + model = AutoModelForCausalLM.from_pretrained(model_id, token=True, load_in_8bit=True, device_map=\"auto\")\n",
        "# if it still does not fit:\n",
        "# + model = AutoModelForCausalLM.from_pretrained(model_id, token=True, load_in_4bit=True, device_map=\"auto\")\n",
        "\n",
        "model = HFModelAdapter(hf_model, model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tEWcAF9zKVCK"
      },
      "outputs": [],
      "source": [
        "# print an example instance of the dataset\n",
        "df = pd.read_csv('./data/itsm_tickets_meaningful_200_utf8.csv', sep=';', encoding='utf-8')\n",
        "\n",
        "# Build disjoint train/test with a shared seed\n",
        "split_seed = 3407\n",
        "train_dataset = CustomDataset(df, 'train', tokenizer, seed=split_seed)\n",
        "test_dataset = CustomDataset(df, 'test', tokenizer, seed=split_seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "96AQg5IQKVCK",
        "outputId": "afe31c3a-36aa-4bcc-bc82-517ab1a8cc9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: tensor([  4684,   3845,  11727,  ..., 151643, 151643, 151643])\n",
            "Labels: tensor([-100, -100, -100,  ..., -100, -100, -100])\n",
            "Decoded Input:\n",
            "description du ticket itsm: Bonjour, je rencontre un problème avec mon lecteur réseau depuis ce matin. Il se met à ne s'affiche pas dans l'explorateur. Pourriez-vous vérifier cela ? Merci d'avance.\n",
            "Réponse de l'équipe IT pour la résolution du ticket: Merci pour votre signalement. Le problème était lié à une panne serveur. Nous avons redémarré le service concerné. Cela devrait être résolu maintenant.\n",
            "\n",
            "Decoded Labels:\n",
            "mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmMerci pour votre signalement. Le problème était lié à une panne serveur. Nous avons redémarré le service concerné. Cela devrait être résolu maintenant.mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Quick check of dataset encoding/decoding process\n",
        "x, y = train_dataset[0]\n",
        "\n",
        "# token ids\n",
        "print(\"Input IDs:\", x)\n",
        "print(\"Labels:\", y)\n",
        "\n",
        "# decoded text\n",
        "print(f\"Decoded Input:\\n{tokenizer.decode(x, skip_special_tokens=True)}\\n\")\n",
        "\n",
        "# For y, I replace all masked tokens (id = -100) by the letter m\n",
        "y = [token if token != -100 else tokenizer.convert_tokens_to_ids('m') for token in y]\n",
        "print(f\"Decoded Labels:\\n{tokenizer.decode(y, skip_special_tokens=True)}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rr31pVlaKVCO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running on device cpu\n"
          ]
        }
      ],
      "source": [
        "train_config = Trainer.get_default_config()\n",
        "train_config.max_iters = 1000\n",
        "train_config.batch_size = 2\n",
        "trainer = Trainer(train_config, model, train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsyvtMXuKVCP",
        "outputId": "c3b23a97-702c-4fe6-9bc7-046b7a524cea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/orfeu/Documents/documents/info_perso/LLM_project_base/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "def batch_end_callback(trainer):\n",
        "    if trainer.iter_num % 100 == 0:\n",
        "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
        "\n",
        "trainer.set_callback('on_batch_end', batch_end_callback)\n",
        "\n",
        "\n",
        "trainer.run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gdKopepZKVCQ"
      },
      "outputs": [],
      "source": [
        "# Quick check to verify the model generates sensible answers\n",
        "def show_prediction_for_row(\n",
        "        row_id,\n",
        "        df,\n",
        "        model,\n",
        "        device,\n",
        "        tokenizer,\n",
        "        train_dataset=None,\n",
        "        max_new_tokens=200,\n",
        "        do_sample=False,\n",
        "        temperature=1.0,\n",
        "        top_k=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Prints the question, ground truth answer, and the model's generated answer for row row_id.\n",
        "    \"\"\"\n",
        "\n",
        "    # Pull the raw texts\n",
        "    question = str(df.loc[row_id, 'question'])\n",
        "    ground_truth = str(df.loc[row_id, 'answer'])\n",
        "\n",
        "    # Re-use the same prompt format as your dataset\n",
        "    if train_dataset is not None and hasattr(train_dataset, \"prompt_description_addition\") and hasattr(train_dataset, \"prompt_resolution_addition\"):\n",
        "        prompt_prefix = train_dataset.prompt_description_addition\n",
        "        between_prefix = train_dataset.prompt_resolution_addition\n",
        "    else:\n",
        "        # Fallbacks in case the dataset is not passed (keep consistent with your training)\n",
        "        prompt_prefix = \"description du ticket itsm: \"\n",
        "        between_prefix = \" Réponse de l'équipe IT pour la résolution du ticket: \"\n",
        "\n",
        "    prompt = f\"{prompt_prefix}{question}{between_prefix}\"\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate_from_prompt(\n",
        "            prompt=prompt,\n",
        "            device=device,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=do_sample,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            return_new_text_only=True,      # only the continuation (answer)\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "    print(f\"Row: {row_id}\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"QUESTION:\")\n",
        "    print(question)\n",
        "    print(\"\\nGROUND TRUTH ANSWER:\")\n",
        "    print(ground_truth)\n",
        "    print(\"\\nMODEL GENERATION:\")\n",
        "    print(generated)\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Example usage (adjust row_id as you like):\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "row_id = 0\n",
        "show_prediction_for_row(row_id, df, model, device, tokenizer, train_dataset=train_dataset,\n",
        "                        max_new_tokens=300, do_sample=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvaluatorEvalCell"
      },
      "outputs": [],
      "source": [
        "eval_config = Evaluator.get_default_config()\n",
        "# keep loader settings consistent with training where useful\n",
        "eval_config.batch_size = train_config.batch_size\n",
        "eval_config.num_workers = train_config.num_workers if hasattr(train_config, 'num_workers') else eval_config.num_workers\n",
        "\n",
        "evaluator = Evaluator(eval_config, model, train_dataset, test_dataset)\n",
        "\n",
        "with torch.no_grad():\n",
        "    train_metrics_eval, _ = evaluator.eval_split('train', max_examples=50, max_new_tokens=200, do_sample=False, print_examples=2)\n",
        "    test_metrics_eval,  _ = evaluator.eval_split('test',  max_examples=50, max_new_tokens=200, do_sample=False, print_examples=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# old code, that does the exact same as the previous cell, delete or ignore it if previous cell worked\n",
        "\n",
        "# import math, random, re, string\n",
        "\n",
        "# def eval_split(\n",
        "#     trainer,\n",
        "#     split='test',\n",
        "#     max_examples=200, # examples on wich generate an answer and compute the metrics, 200 is actually all the dataset\n",
        "#     max_new_tokens=300,\n",
        "#     do_sample=False,\n",
        "#     temperature=1.0,\n",
        "#     top_k=None,\n",
        "#     print_examples=1,\n",
        "#     ):\n",
        "#     \"\"\"\n",
        "#     Evaluate a split on:\n",
        "#       - perplexity over answer bytes (for labels != -100)\n",
        "#       - QA metrics: Exact Match (EM) and token-level F1\n",
        "#       - Generation metric: ROUGE-L (F1)\n",
        "\n",
        "#     Returns: (metrics_dict, examples)\n",
        "#       metrics_dict = { 'byte_perplexity', 'bits_per_byte', 'exact_match', 'token_lvl_f1', 'rougeL_f1', ... }\n",
        "#       examples = list of (question, reference_answer, generated_answer)\n",
        "#     \"\"\"\n",
        "\n",
        "#     model = trainer.model\n",
        "#     device = trainer.device\n",
        "#     dataset = {'train': train_dataset, 'test': test_dataset}[split]\n",
        "#     df = dataset.df\n",
        "#     pad_id = int(model.hf_model.config.pad_token_id)\n",
        "\n",
        "#     model.eval()\n",
        "\n",
        "\n",
        "#     # BPB (bits_per_byte) over the split\n",
        "#     total_nll, total_bytes = 0.0, 0\n",
        "#     loader = DataLoader(dataset, batch_size=trainer.config.batch_size, num_workers=trainer.config.num_workers, drop_last=False)\n",
        "#     with torch.no_grad():\n",
        "#         for x, y in loader:\n",
        "#             x = x.to(device)\n",
        "#             y = y.to(device)\n",
        "#             logits, loss = model(x, y)\n",
        "#             tokens = (y != -100).sum().item()\n",
        "#             if tokens > 0 and loss is not None:\n",
        "#                 # loss.item() is the average negative log-likelihood per included token in the batch\n",
        "#                 # multiply by number of tokens to get total negative log-likelihood for this batch\n",
        "#                 total_nll += loss.item() * tokens\n",
        "\n",
        "#                 # count bytes in the supervised span only\n",
        "#                 # for each sample, take the positions where y != -100 and x != pad\n",
        "#                 for i in range(x.size(0)):\n",
        "#                     mask = (y[i] != -100) & (x[i] != pad_id)\n",
        "#                     if mask.any():\n",
        "#                         ids = x[i][mask].tolist()\n",
        "#                         txt = tokenizer.decode(ids, skip_special_tokens=True)\n",
        "#                         total_bytes += len(txt.encode(\"utf-8\"))\n",
        "\n",
        "#     num_bytes = max(total_bytes, 1)\n",
        "#     bpb = (total_nll / num_bytes) / math.log(2.0)\n",
        "#     byte_perplexity = 2 ** bpb\n",
        "\n",
        "#     # standardize the text (lowercase, removes punctuation, removes extra whitespace)\n",
        "#     def normalize_text(s):\n",
        "#         if s is None:\n",
        "#             return ''\n",
        "#         s = s.strip().lower()\n",
        "#         s = s.translate(str.maketrans('', '', string.punctuation))\n",
        "#         s = re.sub(r'\\s+', ' ', s)\n",
        "#         return s\n",
        "\n",
        "#     def token_lvl_f1_score(prediction, ground_truth):\n",
        "#         pred_tokens = normalize_text(prediction).split()\n",
        "#         gt_tokens = normalize_text(ground_truth).split()\n",
        "#         if len(pred_tokens) == 0 and len(gt_tokens) == 0:\n",
        "#             return 1.0\n",
        "#         # count overlaps (bag-of-words)\n",
        "#         from collections import Counter\n",
        "#         pred_counts = Counter(pred_tokens)\n",
        "#         gt_counts = Counter(gt_tokens)\n",
        "#         overlap = sum((pred_counts & gt_counts).values())\n",
        "#         if overlap == 0:\n",
        "#             return 0.0\n",
        "#         precision = overlap / max(len(pred_tokens), 1)\n",
        "#         recall = overlap / max(len(gt_tokens), 1)\n",
        "#         return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "#     def exact_match(prediction, ground_truth):\n",
        "#         return 1.0 if normalize_text(prediction) == normalize_text(ground_truth) else 0.0\n",
        "\n",
        "#     # Dynamic programming algorithm to find the length of the Longest Common Subsequence (LCS)\n",
        "#     def lcs(x, y):\n",
        "#         m, n = len(x), len(y)\n",
        "#         dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "#         for i in range(m):\n",
        "#             xi = x[i]\n",
        "#             dpi = dp[i]\n",
        "#             dpi1 = dp[i+1]\n",
        "#             for j in range(n):\n",
        "#                 if xi == y[j]:\n",
        "#                     dpi1[j+1] = dpi[j] + 1\n",
        "#                 else:\n",
        "#                     dpi1[j+1] = dpi1[j] if dpi1[j] >= dp[i][j+1] else dp[i][j+1]\n",
        "#         return dp[m][n]\n",
        "\n",
        "#     def rougeL_f1(prediction, ground_truth):\n",
        "#         pred_tokens = normalize_text(prediction).split()\n",
        "#         gt_tokens = normalize_text(ground_truth).split()\n",
        "#         if len(pred_tokens) == 0 or len(gt_tokens) == 0:\n",
        "#             return 0.0\n",
        "#         # length of the Longest Common Subsequence (LCS)\n",
        "#         # (orderded well predicted tokens, not necessarily consecutive)\n",
        "#         lcs_len = lcs(pred_tokens, gt_tokens)\n",
        "#         prec = lcs_len / len(pred_tokens)\n",
        "#         rec = lcs_len / len(gt_tokens)\n",
        "#         if prec + rec == 0:\n",
        "#             return 0.0\n",
        "#         return (2 * prec * rec) / (prec + rec)\n",
        "\n",
        "#     # Generation loop for QA metrics\n",
        "#     total_exact_match, total_f1, total_rougeL = 0.0, 0.0, 0.0\n",
        "#     num_examples = min(max_examples, len(dataset))\n",
        "#     indices = list(range(len(dataset)))\n",
        "#     random.seed(3407)\n",
        "#     random.shuffle(indices)\n",
        "#     indices = indices[:num_examples]\n",
        "\n",
        "#     examples = []  # (question, reference, generated)\n",
        "#     with torch.no_grad():\n",
        "#         for i_local in indices:\n",
        "#             row_idx = int(dataset.ixes[i_local])\n",
        "#             question = str(df.loc[row_idx, 'question'])\n",
        "#             reference = str(df.loc[row_idx, 'answer'])\n",
        "#             prompt = dataset.prompt_description_addition + question + dataset.prompt_resolution_addition\n",
        "\n",
        "#             generated = model.generate_from_prompt(\n",
        "#                 prompt=prompt,\n",
        "#                 device=device,\n",
        "#                 max_new_tokens=max_new_tokens,\n",
        "#                 do_sample=do_sample,\n",
        "#                 temperature=temperature,\n",
        "#                 top_k=top_k,\n",
        "#                 return_new_text_only=True,\n",
        "#                 skip_special_tokens=True,\n",
        "#             )\n",
        "\n",
        "#             examples.append((question, reference, generated))\n",
        "#             total_exact_match += exact_match(generated, reference)\n",
        "#             total_f1 += token_lvl_f1_score(generated, reference)\n",
        "#             total_rougeL += rougeL_f1(generated, reference)\n",
        "\n",
        "#     qa_em = total_exact_match / max(num_examples, 1)\n",
        "#     qa_f1 = total_f1 / max(num_examples, 1)\n",
        "#     rougeL = total_rougeL / max(num_examples, 1)\n",
        "\n",
        "#     results = {\n",
        "#         'split': split,\n",
        "#         'examples_evaluated': int(num_examples),\n",
        "#         'byte_perplexity': float(byte_perplexity) if total_bytes > 0 else None,\n",
        "#         'bits_per_byte': float(bpb) if total_bytes > 0 else None,\n",
        "#         'exact_match': float(qa_em),\n",
        "#         'token_lvl_f1': float(qa_f1),\n",
        "#         'rougeL_f1': float(rougeL),\n",
        "#     }\n",
        "\n",
        "#     if print_examples > 0:\n",
        "#         for k, (q, ref, pred) in enumerate(examples[:print_examples]):\n",
        "#             print(f'[#{k}] QUESTION: {q}')\n",
        "#             print(f'     REF    : {ref}')\n",
        "#             print(f'     PRED   : {pred}')\n",
        "#             print('-' * 60)\n",
        "\n",
        "#     print(\n",
        "#         f\"Eval {split}: byte_perplexity={results['byte_perplexity']:.3f} | exact match={qa_em*100:.2f}% | token level f1={qa_f1*100:.2f}% | ROUGE-L={rougeL*100:.2f}% | examples={num_examples}\"\n",
        "#     )\n",
        "#     return results, examples\n",
        "\n",
        "# # Example: evaluate both splits\n",
        "# with torch.no_grad():\n",
        "#     train_metrics, _ = eval_split(trainer, 'train', max_examples=50, max_new_tokens=200, do_sample=False, print_examples=2)\n",
        "#     test_metrics, _  = eval_split(trainer, 'test',  max_examples=50, max_new_tokens=200, do_sample=False, print_examples=2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
